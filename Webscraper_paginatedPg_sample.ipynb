{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfde018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  References  ##########\n",
    "# https://medium.com/codex/web-scraping-paginated-webpages-with-python-selenium-and-beautifulsoup4-8b415f833132\n",
    "# Youtube plus Github - https://www.youtube.com/c/JohnWatsonRooney ; https://github.com/jhnwr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f69fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import re\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99206b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = webdriver.ChromeService() #https://www.selenium.dev/documentation/webdriver/drivers/service/\n",
    "options = Options()\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('start-maximized')\n",
    "driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the blog\n",
    "BASE_URL = 'https://xxx.ca/blog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98afbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape a single blog page\n",
    "def scrape_blog_page(blog_url):\n",
    "    driver.get(blog_url)\n",
    "    html = driver.page_source\n",
    "    soup = bs(html, features=\"html.parser\")\n",
    "    \n",
    "    # Extract page title\n",
    "    title_tag = soup.find(\"div\", class_=\"section-title\")\n",
    "    page_title = (\n",
    "        title_tag.find(\"h3\").get_text(strip=True)\n",
    "        if title_tag and title_tag.find(\"h3\")\n",
    "        else \"Untitled\"\n",
    "    )\n",
    "    \n",
    "    # Extract image URL\n",
    "    image_tag = soup.find(\"div\", class_=\"col-lg-7\")\n",
    "    image_url = (\n",
    "        image_tag.find(\"img\")[\"src\"] if image_tag and image_tag.find(\"img\") else None\n",
    "    )\n",
    "    if image_url and not image_url.startswith(\"http\"):\n",
    "        image_url = f\"https://xxx.ca{image_url}\"  # Convert to absolute URL\n",
    "    \n",
    "    # Extract blog content\n",
    "    content_tag = soup.find(\"div\", class_=\"col-lg-12\")\n",
    "    blog_content = content_tag.find(\"div\", class_=\"section-text my-5\") if content_tag else None\n",
    "    \n",
    "    if blog_content:\n",
    "        # Remove the social share div\n",
    "        social_share = blog_content.find(\"div\", class_=\"social-share\")\n",
    "        if social_share:\n",
    "            social_share.decompose()  # Remove the div and its contents\n",
    "        \n",
    "        # Remove class attributes from the content\n",
    "        for tag in blog_content.find_all(True):  # Finds all child tags\n",
    "            tag.attrs = {}\n",
    "        content_html = str(blog_content)\n",
    "    else:\n",
    "        content_html = \"\"\n",
    "    \n",
    "    return {\n",
    "        \"url\": blog_url,\n",
    "        \"title\": page_title,\n",
    "        \"image_url\": image_url,\n",
    "        \"content_html\": content_html,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b01ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape blog URLs from a single page\n",
    "def scrape_page_for_links(page_url):\n",
    "    driver.get(page_url)\n",
    "    html = driver.page_source\n",
    "    soup = bs(html, features=\"html.parser\")\n",
    "    \n",
    "    # Extract blog links from div tags with class 'col-md-6'\n",
    "    links = [\n",
    "        box.find(\"a\", href=True)[\"href\"]\n",
    "        for box in soup.find_all(\"div\", class_=\"col-md-6\")\n",
    "    ]\n",
    "    # Convert relative links to absolute if needed\n",
    "    links = [\n",
    "        link if link.startswith(\"http\") else f\"https://xxx.ca{link}\" for link in links\n",
    "    ]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all paginated pages\n",
    "def scrape_paginated_website(start_page, end_page):\n",
    "    for page_number in range(start_page, end_page + 1):\n",
    "        # Construct page URL\n",
    "        page_url = BASE_URL if page_number == 1 else f\"{BASE_URL}/p{page_number}\"\n",
    "        print(f\"Scraping page: {page_url}\")\n",
    "        \n",
    "        # Create subfolder for the page\n",
    "        folder_name = f\"BlogPage{page_number}\"\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "        \n",
    "        # Get all blog links on the current page\n",
    "        blog_links = scrape_page_for_links(page_url)\n",
    "        \n",
    "        for blog_url in blog_links:\n",
    "            print(f\"Scraping blog: {blog_url}\")\n",
    "            blog_data = scrape_blog_page(blog_url)\n",
    "            \n",
    "            # Save blog content to an HTML file in the subfolder\n",
    "            blog_filename = os.path.join(folder_name, f\"{blog_url.split('/')[-1]}.html\")\n",
    "            with open(blog_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(\"<html><body>\")\n",
    "                # Make the H1 tag a hyperlink to the blog URL\n",
    "                file.write(f\"<h1><a href='{blog_data['url']}'>{blog_data['title']}</a></h1>\")\n",
    "                if blog_data[\"image_url\"]:\n",
    "                    file.write(f\"<img src='{blog_data['image_url']}' alt='Blog Image'>\")\n",
    "                file.write(blog_data[\"content_html\"])\n",
    "                file.write(\"</body></html>\")\n",
    "    print(\"All blogs scraped and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        START_PAGE = 1\n",
    "        END_PAGE = 6  # Adjust based on the total number of pages\n",
    "        scrape_paginated_website(START_PAGE, END_PAGE)\n",
    "        print(\"Scraping completed! Blog HTML files saved in respective subfolders.\")\n",
    "    finally:\n",
    "        driver.quit()  # Ensure the driver is closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110fad9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
